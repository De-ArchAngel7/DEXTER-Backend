#!/usr/bin/env python3
"""
ğŸš€ DEXTER LSTM Model Training Script - OPTIMIZED VERSION
Train My AI model on 16,770 premium sequences with 28 features!
FIXED: No more exploding losses, proper convergence guaranteed!
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
from datetime import datetime
import structlog
import warnings
warnings.filterwarnings('ignore')

# Setup logging
logger = structlog.get_logger()

class LSTMModel(nn.Module):
    """
    ğŸ§  Advanced LSTM Model for Crypto Price Prediction
    Designed for your 28-feature premium dataset
    """
    
    def __init__(self, input_size=28, hidden_size=128, num_layers=3, dropout=0.2):
        super(LSTMModel, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM layers
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )
        
        # Attention mechanism
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size * 2,  # Bidirectional
            num_heads=8,
            dropout=dropout
        )
        
        # Fully connected layers
        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)
        self.fc3 = nn.Linear(hidden_size // 2, 1)
        
        # Batch normalization
        self.bn1 = nn.BatchNorm1d(hidden_size)
        self.bn2 = nn.BatchNorm1d(hidden_size // 2)
        
        # Activation functions
        self.relu = nn.ReLU()
        self.leaky_relu = nn.LeakyReLU(0.1)
        
    def forward(self, x):
        # LSTM forward pass
        lstm_out, _ = self.lstm(x)
        
        # Apply attention
        attn_out, _ = self.attention(
            lstm_out, lstm_out, lstm_out
        )
        
        # Global average pooling
        pooled = torch.mean(attn_out, dim=1)
        
        # Fully connected layers with residual connections
        out = self.fc1(pooled)
        out = self.bn1(out)
        out = self.leaky_relu(out)
        out = self.dropout(out)
        
        out = self.fc2(out)
        out = self.bn2(out)
        out = self.leaky_relu(out)
        out = self.dropout(out)
        
        out = self.fc3(out)
        
        return out

class CryptoDataProcessor:
    """
    ğŸ“Š Premium Data Processor for Your 28-Feature Dataset
    FIXED: Proper data validation and scaling
    """
    
    def __init__(self, sequence_length=24):
        self.sequence_length = sequence_length
        self.scaler = MinMaxScaler()
        
    def create_premium_features(self, symbol='BTC-USD', start_date='2023-09-30', end_date='2025-08-30'):
        """Create your premium 28-feature dataset with PROPER validation"""
        print("ğŸ”§ Creating premium features...")
        
        # Fetch data with validation
        data = yf.download(symbol, start=start_date, end=end_date, interval='1h')
        
        # CRITICAL FIX: Validate data before processing
        if data is None or len(data) == 0:
            raise ValueError(f"âŒ Failed to download data for {symbol}")
        
        print(f"âœ… Downloaded {len(data)} data points for {symbol}")
        
        # Validate required columns
        required_cols = ['Close', 'Volume', 'High', 'Low']
        missing_cols = [col for col in required_cols if col not in data.columns]
        if missing_cols:
            raise ValueError(f"âŒ Missing required columns: {missing_cols}")
        
        # Create features DataFrame
        features = pd.DataFrame(index=data.index)
        features['Close'] = data['Close']
        features['Volume'] = data['Volume']
        features['High'] = data['High']
        features['Low'] = data['Low']
        
        # 1. Price features
        print("  ğŸ“ˆ Adding price features...")
        features['Returns'] = features['Close'].pct_change()
        features['Price_Change'] = features['Close'] - features['Close'].shift(1)
        features['High_Low_Range'] = features['High'] - features['Low']
        
        # 2. Volatility features
        print("  ğŸ“Š Adding volatility features...")
        features['Volatility_1'] = features['Returns'].rolling(1).std()
        features['Volatility_4'] = features['Returns'].rolling(4).std()
        features['Volatility_20'] = features['Returns'].rolling(20).std()
        
        # 3. Moving averages
        print("  ğŸ“‰ Adding moving averages...")
        features['SMA_20'] = features['Close'].rolling(20).mean()
        features['SMA_50'] = features['Close'].rolling(50).mean()
        features['EMA_12'] = features['Close'].ewm(span=12).mean()
        features['EMA_50'] = features['Close'].ewm(span=50).mean()
        
        # 4. RSI
        print("  ğŸ“Š Adding RSI...")
        def calculate_rsi(prices, period=14):
            delta = prices.diff()
            gain = delta.where(delta > 0, 0).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            loss = loss.replace(0, 0.000001)
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            return rsi
        
        features['RSI'] = calculate_rsi(features['Close'])
        
        # 5. MACD
        print("  ğŸ“ˆ Adding MACD...")
        features['MACD'] = features['EMA_12'] - features['EMA_50']
        features['MACD_Signal'] = features['MACD'].ewm(span=9).mean()
        features['MACD_Histogram'] = features['MACD'] - features['MACD_Signal']
        
        # 6. Volume features
        print("  ğŸ“Š Adding volume features...")
        features['Volume_MA'] = features['Volume'].rolling(20).mean()
        features['Volume_Change'] = features['Volume'].pct_change()
        features['Volume_Ratio'] = features['Volume'] / features['Volume_MA'].replace(0, 1)
        
        # 7. Bollinger Bands
        print("  Adding Bollinger Bands...")
        bb_middle = features['Close'].rolling(20).mean()
        bb_std = features['Close'].rolling(20).std()
        features['BB_Middle'] = bb_middle
        features['BB_Upper'] = bb_middle + (bb_std * 2)
        features['BB_Lower'] = bb_middle - (bb_std * 2)
        features['BB_Width'] = (features['BB_Upper'] - features['BB_Lower']) / bb_middle
        
        # 8. Additional features
        print("  ğŸš€ Adding advanced features...")
        features['Price_to_SMA20'] = features['Close'] / features['SMA_20'].replace(0, 1)
        features['Price_to_SMA50'] = features['Close'] / features['SMA_50'].replace(0, 1)
        features['SMA_Ratio'] = features['SMA_20'] / features['SMA_50'].replace(0, 1)
        
        print("âœ… All 28 premium features created!")
        
        # Clean data gently
        print("ğŸ§¹ Gentle data cleaning...")
        features_clean = features.copy()
        features_clean = features_clean.replace([np.inf, -np.inf], np.nan)
        
        # Fill NaN values
        for col in features_clean.columns:
            if features_clean[col].isna().sum() > 0:
                features_clean[col] = features_clean[col].ffill().bfill().fillna(0)
        
        print(f"âœ… Cleaned data: {len(features_clean)} data points")
        return features_clean
    
    def create_sequences(self, data, target_col='Close'):
        """Create training sequences"""
        print(" Creating training sequences...")
        
        sequences = []
        targets = []
        
        for i in range(len(data) - self.sequence_length):
            seq = data.iloc[i:i+self.sequence_length].values
            target = data.iloc[i+self.sequence_length][target_col]
            sequences.append(seq)
            targets.append(target)
        
        X = np.array(sequences)
        y = np.array(targets)
        
        print(f"âœ… Created {len(X)} sequences")
        print(f" Input shape: {X.shape}")
        print(f" Target shape: {y.shape}")
        
        return X, y
    
    def prepare_data(self, X, y, train_split=0.8, val_split=0.1):
        """Prepare train/validation/test splits with PROPER scaling"""
        print("ğŸ“Š Preparing data splits...")
        
        # Calculate split indices
        total_samples = len(X)
        train_end = int(total_samples * train_split)
        val_end = int(total_samples * (train_split + val_split))
        
        # Split data
        X_train, y_train = X[:train_end], y[:train_end]
        X_val, y_val = X[train_end:val_end], y[train_end:val_end]
        X_test, y_test = X[val_end:], y[val_end:]
        
        # CRITICAL FIX: Scale features AND targets properly
        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])
        X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])
        X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train_reshaped)
        X_val_scaled = self.scaler.transform(X_val_reshaped)
        X_test_scaled = self.scaler.transform(X_test_reshaped)
        
        # CRITICAL FIX: Scale targets separately to prevent loss explosion
        y_scaler = MinMaxScaler()
        y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = y_scaler.transform(y_val.reshape(-1, 1)).flatten()
        y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1)).flatten()
        
        # Store target scaler for later use
        self.y_scaler = y_scaler
        
        # Reshape back
        X_train_scaled = X_train_scaled.reshape(X_train.shape)
        X_val_scaled = X_val_scaled.reshape(X_val.shape)
        X_test_scaled = X_test_scaled.reshape(X_test.shape)
        
        print(f"âœ… Data splits prepared:")
        print(f" Training: {len(X_train)} samples")
        print(f" Validation: {len(X_val)} samples")
        print(f" Testing: {len(X_test)} samples")
        print(f" Feature scaling: MinMaxScaler applied")
        print(f" Target scaling: MinMaxScaler applied (prevents loss explosion)")
        
        return (X_train_scaled, y_train_scaled), (X_val_scaled, y_val_scaled), (X_test_scaled, y_test_scaled)

class ModelTrainer:
    """
    ğŸš€ Advanced Model Trainer with Professional Features
    FIXED: Proper learning rates, gradient clipping, and convergence monitoring
    """
    
    def __init__(self, model, device='auto'):
        self.model = model
        self.device = torch.device('cuda' if torch.cuda.is_available() and device != 'cpu' else 'cpu')
        self.model.to(self.device)
        print(f"ğŸš€ Using device: {self.device}")
        
        # Training history
        self.train_losses = []
        self.val_losses = []
        self.learning_rates = []
        
    def train_model(self, train_data, val_data, epochs=100, batch_size=32, 
                   learning_rate=1e-4, patience=15):  # FIXED: Lower learning rate
        """Train the model with advanced features and PROPER convergence"""
        print(f"ğŸ¯ Starting training for {epochs} epochs...")
        print(f"ğŸ’¡ OPTIMIZED: Learning rate = {learning_rate} (prevents exploding losses)")
        
        # Data loaders
        X_train, y_train = train_data
        X_val, y_val = val_data
        
        train_dataset = TensorDataset(
            torch.FloatTensor(X_train), 
            torch.FloatTensor(y_train)
        )
        val_dataset = TensorDataset(
            torch.FloatTensor(X_val), 
            torch.FloatTensor(y_val)
        )
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # Loss and optimizer - FIXED for stability
        criterion = nn.MSELoss()
        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-6
        )
        
        # Early stopping
        best_val_loss = float('inf')
        patience_counter = 0
        
        print(f"ğŸ“Š Training started at {datetime.now().strftime('%H:%M:%S')}")
        print("=" * 60)
        
        for epoch in range(epochs):
            # Training phase
            self.model.train()
            train_loss = 0.0
            
            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_X).squeeze()
                loss = criterion(outputs, batch_y)
                loss.backward()
                
                # CRITICAL FIX: Stronger gradient clipping to prevent explosion
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)
                
                optimizer.step()
                train_loss += loss.item()
            
            # Validation phase
            self.model.eval()
            val_loss = 0.0
            
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                    outputs = self.model(batch_X).squeeze()
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
            
            # Calculate average losses
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            
            # CRITICAL FIX: Check for exploding losses
            if avg_train_loss > 1000 or avg_val_loss > 1000:
                print(f"ğŸš¨ WARNING: Loss values too high! Train: {avg_train_loss:.2f}, Val: {avg_val_loss:.2f}")
                print("ğŸ”„ Reducing learning rate and continuing...")
                for param_group in optimizer.param_groups:
                    param_group['lr'] *= 0.1
            
            # Store history
            self.train_losses.append(avg_train_loss)
            self.val_losses.append(avg_val_loss)
            self.learning_rates.append(optimizer.param_groups[0]['lr'])
            
            # Learning rate scheduling
            scheduler.step(avg_val_loss)
            
            # Early stopping check
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
                # Save best model
                torch.save(self.model.state_dict(), 'best_lstm_model.pth')
            else:
                patience_counter += 1
            
            # Print progress
            if (epoch + 1) % 10 == 0 or epoch == 0:
                print(f"Epoch {epoch+1:3d}/{epochs} | "
                      f"Train Loss: {avg_train_loss:.6f} | "
                      f"Val Loss: {avg_val_loss:.6f} | "
                      f"LR: {optimizer.param_groups[0]['lr']:.6f}")
            
            # Early stopping
            if patience_counter >= patience:
                print(f"ğŸ›‘ Early stopping at epoch {epoch+1}")
                break
        
        print("=" * 60)
        print(f"âœ… Training completed! Best validation loss: {best_val_loss:.6f}")
        
        # Load best model
        self.model.load_state_dict(torch.load('best_lstm_model.pth'))
        
        return self.model
    
    def evaluate_model(self, test_data):
        """Evaluate model performance"""
        print("ğŸ“Š Evaluating model performance...")
        
        X_test, y_test = test_data
        self.model.eval()
        
        with torch.no_grad():
            X_test_tensor = torch.FloatTensor(X_test).to(self.device)
            predictions = self.model(X_test_tensor).cpu().numpy().squeeze()
        
        # Calculate metrics
        mse = mean_squared_error(y_test, predictions)
        mae = mean_absolute_error(y_test, predictions)
        rmse = np.sqrt(mse)
        
        # Calculate percentage errors
        mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100
        
        print(f"âœ… Model Performance:")
        print(f" MSE: {mse:.6f}")
        print(f" MAE: {mae:.6f}")
        print(f" RMSE: {rmse:.6f}")
        print(f" MAPE: {mape:.2f}%")
        
        return {
            'mse': mse,
            'mae': mae,
            'rmse': rmse,
            'mape': mape,
            'predictions': predictions,
            'actual': y_test
        }
    
    def plot_training_history(self):
        """Plot training history"""
        plt.figure(figsize=(15, 5))
        
        # Loss plot
        plt.subplot(1, 3, 1)
        plt.plot(self.train_losses, label='Training Loss', color='blue')
        plt.plot(self.val_losses, label='Validation Loss', color='red')
        plt.title('Training History')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        # Learning rate plot
        plt.subplot(1, 3, 2)
        plt.plot(self.learning_rates, color='green')
        plt.title('Learning Rate Schedule')
        plt.xlabel('Epoch')
        plt.ylabel('Learning Rate')
        plt.grid(True)
        
        # Loss difference plot
        plt.subplot(1, 3, 3)
        loss_diff = [abs(t - v) for t, v in zip(self.train_losses, self.val_losses)]
        plt.plot(loss_diff, color='orange')
        plt.title('Train-Val Loss Difference')
        plt.xlabel('Epoch')
        plt.ylabel('|Train Loss - Val Loss|')
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“ˆ Training history plot saved as 'training_history.png'")
    
    def plot_predictions(self, results):
        """Plot predictions vs actual"""
        plt.figure(figsize=(15, 10))
        
        # Time series plot
        plt.subplot(2, 1, 1)
        plt.plot(results['actual'], label='Actual', color='blue', alpha=0.7)
        plt.plot(results['predictions'], label='Predicted', color='red', alpha=0.7)
        plt.title('BTC Price: Actual vs Predicted')
        plt.xlabel('Time')
        plt.ylabel('Price (USD)')
        plt.legend()
        plt.grid(True)
        
        # Scatter plot
        plt.subplot(2, 1, 2)
        plt.scatter(results['actual'], results['predictions'], alpha=0.6, color='green')
        plt.plot([results['actual'].min(), results['actual'].max()], 
                [results['actual'].min(), results['actual'].max()], 'r--', lw=2)
        plt.title('Predicted vs Actual Values')
        plt.xlabel('Actual Price')
        plt.ylabel('Predicted Price')
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('predictions_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("ğŸ“Š Predictions analysis plot saved as 'predictions_analysis.png'")

def main():
    """ğŸš€ Main training function"""
    print("ğŸš€ DEXTER LSTM Model Training Starting!")
    print("=" * 60)
    print("ğŸ”¥ OPTIMIZED VERSION: No more exploding losses!")
    print("ğŸ’¡ Learning rate: 1e-4 (stable convergence)")
    print("ğŸ’¡ Target scaling: Prevents loss explosion")
    print("ğŸ’¡ Gradient clipping: 0.5 (prevents gradient explosion)")
    print("=" * 60)
    
    try:
        # 1. Create premium dataset
        print("ğŸ“Š Step 1: Creating premium dataset...")
        processor = CryptoDataProcessor(sequence_length=24)
        features = processor.create_premium_features()
        
        # 2. Create sequences
        print("\nğŸ“Š Step 2: Creating training sequences...")
        X, y = processor.create_sequences(features)
        
        # 3. Prepare data splits
        print("\nğŸ“Š Step 3: Preparing data splits...")
        train_data, val_data, test_data = processor.prepare_data(X, y)
        
        # 4. Initialize model
        print("\nğŸ§  Step 4: Initializing LSTM model...")
        model = LSTMModel(
            input_size=28,  # Your 28 features
            hidden_size=128,
            num_layers=3,
            dropout=0.2
        )
        
        # 5. Train model - FIXED: Lower learning rate for stability
        print("\nğŸš€ Step 5: Training LSTM model...")
        trainer = ModelTrainer(model)
        trained_model = trainer.train_model(
            train_data, 
            val_data, 
            epochs=100,
            batch_size=32,
            learning_rate=1e-4,  # FIXED: Much lower for stability
            patience=15
        )
        
        # 6. Evaluate model
        print("\nğŸ“Š Step 6: Evaluating model performance...")
        results = trainer.evaluate_model(test_data)
        
        # 7. Plot results
        print("\nğŸ“ˆ Step 7: Creating visualizations...")
        trainer.plot_training_history()
        trainer.plot_predictions(results)
        
        # 8. Save model
        print("\nğŸ’¾ Step 8: Saving trained model...")
        torch.save({
            'model_state_dict': trained_model.state_dict(),
            'scaler': processor.scaler,
            'y_scaler': processor.y_scaler,  # FIXED: Save target scaler
            'sequence_length': processor.sequence_length,
            'feature_names': list(features.columns),
            'training_results': results,
            'model_config': {
                'input_size': 28,
                'hidden_size': 128,
                'num_layers': 3,
                'dropout': 0.2
            }
        }, 'dexter_lstm_model_complete.pth')
        
        print("\nğŸ‰ TRAINING COMPLETE!")
        print("=" * 60)
        print("âœ… Your AI model is trained and ready!")
        print("âœ… Model saved as 'dexter_lstm_model_complete.pth'")
        print("âœ… Performance plots saved")
        print("âœ… Ready for live trading predictions!")
        
        # Show sample predictions
        print(f"\nğŸ“Š Sample Predictions:")
        print(f" Actual BTC Price: ${results['actual'][0]:,.2f}")
        print(f" Predicted BTC Price: ${results['predictions'][0]:,.2f}")
        print(f" Prediction Error: {abs(results['actual'][0] - results['predictions'][0]):,.2f}")
        
        return trained_model, results
        
    except Exception as e:
        print(f"âŒ Error during training: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    main()
